"""
CNN DO ROZPOZNAWANIA GATUNK√ìW THERAPHOSIDAE
        Pawe≈Ç Grygielski(121678)
     MODEL PRETRENOWANY Z INCEPTIONV3
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# --- ≈öcie≈ºki do zbior√≥w treningowego i testowego ---
train_path = 'C:/Users/pgryg/Desktop/CNN_project/Species'
test_path = 'C:/Users/pgryg/Desktop/CNN_project/test'

# --- Generatory obraz√≥w ---
# Dla InceptionV3 wymagane jest preprocessing_function=preprocess_input,
# kt√≥re normalizuje obrazy tak jak podczas treningu InceptionV3 na ImageNet.

train_datagen_inception = ImageDataGenerator(
    preprocessing_function=preprocess_input,  # preprocessowanie pod InceptionV3
    shear_range=0.2,                         # losowe przesuniƒôcie obrazu wzd≈Çu≈º osi X
    zoom_range=0.2,                          # losowe przybli≈ºanie obrazu
    horizontal_flip=True,                    # losowe odbicia lustrzane poziome
    rotation_range=30,                       # losowe rotacje do ¬±30 stopni
    width_shift_range=0.2,                   # losowe przesuniƒôcia w osi X (20% szeroko≈õci)
    height_shift_range=0.2,                  # losowe przesuniƒôcia w osi Y (20% wysoko≈õci)
    brightness_range=[0.8, 1.2],             # losowa zmiana jasno≈õci obrazu
    channel_shift_range=10.0                  # losowe przesuniƒôcie warto≈õci kana≈Ç√≥w RGB
)

test_datagen_inception = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

# Tworzenie generatora danych treningowych
train_generator_inception = train_datagen_inception.flow_from_directory(
    train_path,
    target_size=(299, 299),  # InceptionV3 wymaga wej≈õcia 299x299
    batch_size=32,
    class_mode='categorical' # klasyfikacja wieloklasowa - one-hot encoding etykiet
)

# Tworzenie generatora danych testowych - bez shuffle, aby zachowaƒá kolejno≈õƒá
test_generator_inception = test_datagen_inception.flow_from_directory(
    test_path,
    target_size=(299, 299),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# --- Budowa modelu ---
# Wykorzystuje gotowy model InceptionV3 jako ekstraktor cech (base_model),
# wczytujƒÖc wagi wytrenowane na ImageNet.
# include_top=False oznacza, ≈ºe nie u≈ºywa domy≈õlnej gƒôstej warstwy klasyfikacyjnej InceptionV3.

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))
base_model.trainable = False  # zamra≈ºenie warstw bazowych - nie bƒôdƒÖ trenowane

# Tworzenie modelu sekwencyjnego na bazie ekstraktora cech
model_inception = models.Sequential([
    base_model,  # ekstrakcja cech z InceptionV3

    # GlobalAveragePooling2D - zamiast sp≈Çaszczaƒá feature mapy, 
    # u≈õrednia warto≈õci cech w ka≈ºdej mapie, zmniejszajƒÖc liczbƒô parametr√≥w i przeciwdzia≈ÇajƒÖc przeuczeniu.
    layers.GlobalAveragePooling2D(),

    # Gƒôsta warstwa ukryta z 512 neuronami i aktywacjƒÖ ReLU,
    # pozwalajƒÖca modelowi uczyƒá siƒô bardziej z≈Ço≈ºonych reprezentacji.
    layers.Dense(512, activation='relu'),

    # Normalizacja batchy - pomaga stabilizowaƒá i przyspieszaƒá trening
    layers.BatchNormalization(),

    # Dropout 0.5 - zapobiega przeuczeniu przez losowe "wy≈ÇƒÖczanie" po≈Çowy neuron√≥w podczas treningu
    layers.Dropout(0.5),

    # Ostatnia warstwa gƒôsta z 101 neuronami (liczba klas),
    # aktywacja softmax zapewnia, ≈ºe wyj≈õcia to prawdopodobie≈Ñstwa klas.
    layers.Dense(101, activation='softmax')
])

# --- Kompilacja modelu ---
# U≈ºywacie optymalizatora Adam, kt√≥ry adaptuje wsp√≥≈Çczynnik uczenia.
# Funkcja straty categorical_crossentropy jest standardowa dla klasyfikacji wieloklasowej.
model_inception.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']  # podczas treningu monitorujemy dok≈Çadno≈õƒá
)

# --- Definicja callback√≥w ---
# EarlyStopping - zatrzymuje trening je≈õli przez 20 epok nie poprawi siƒô 'val_loss'
early_stop_inception = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

# ModelCheckpoint - zapisuje najlepszy model na dysku, monitorujƒÖc 'val_loss'
model_checkpoint_inception = callbacks.ModelCheckpoint(
    'C:/Users/pgryg/Desktop/CNN_project/best_model_inception.h5',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# ReduceLROnPlateau - zmniejsza LR je≈õli 'val_loss' przestaje siƒô poprawiaƒá przez 5 epok
reduce_lr_inception = callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

# --- Trenowanie modelu ---
epochs = 150
history_inception = model_inception.fit(
    train_generator_inception,
    epochs=epochs,
    validation_data=test_generator_inception,
    callbacks=[early_stop_inception, model_checkpoint_inception, reduce_lr_inception]
)

# --- FINE-TUNING INCEPTIONV3 ---
# Odblokowujemy czƒô≈õƒá warstw modelu bazowego w celu dalszego dopasowania do konkretnego zbioru danych.
# To tzw. fine-tuning ‚Äì dalsze dostrajanie wag wcze≈õniej wytrenowanego modelu.
# Te warstwy na wcze≈õniejszych etapach treningu by≈Çy zamro≈ºone w 68 linijce poprzez base_model.trainable = False 

base_model.trainable = True  # Odblokowuje wszystkie warstwy bazowe

# Sprawdzenie ile warstw ma model bazowy
print(f"\n‚û°Ô∏è Liczba warstw w modelu bazowym InceptionV3: {len(base_model.layers)}")

# Zamra≈ºa pierwsze 250 warstw ‚Äì nie bƒôdƒÖ trenowane.
# Pozwala to zachowaƒá og√≥lne cechy wyuczone na ImageNet, 
# jednocze≈õnie umo≈ºliwiajƒÖc naukƒô bardziej specyficznych cech dla nowego zbioru danych w dalszych warstwach.
for layer in base_model.layers[:250]:
    layer.trainable = False

# Ponowna kompilacja modelu z mniejszym learning rate
# (wa≈ºne przy fine-tuningu, aby nie "zepsuƒá" wcze≈õniej wyuczonych wag du≈ºymi zmianami).
model_inception.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callback do zapisywania najlepszego modelu podczas fine-tuningu
model_checkpoint_finetune = callbacks.ModelCheckpoint(
    'C:/Users/pgryg/Desktop/CNN_project/best_model_inception_finetuned.h5',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# --- Trening modelu z odblokowanymi warstwami (fine-tuning) ---
# Kontynuujemy trenowanie przez dodatkowe 150 epok, teraz uczƒÖc model bazowy InceptionV3.
fine_tune_epochs = 150
history_finetune = model_inception.fit(
    train_generator_inception,
    epochs=fine_tune_epochs,
    validation_data=test_generator_inception,
    callbacks=[early_stop_inception, model_checkpoint_finetune, reduce_lr_inception]
)

# --- Wizualizacja wynik√≥w po fine-tuningu ---
plt.figure(figsize=(12,5))

# Dok≈Çadno≈õƒá
plt.subplot(1,2,1)
plt.plot(history_finetune.history['accuracy'], label='Train Accuracy')
plt.plot(history_finetune.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy after Fine-tuning (InceptionV3)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Strata
plt.subplot(1,2,2)
plt.plot(history_finetune.history['loss'], label='Train Loss')
plt.plot(history_finetune.history['val_loss'], label='Val Loss')
plt.title('Model Loss after Fine-tuning (InceptionV3)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('C:/Users/pgryg/Desktop/CNN_project/training_plot_inception_finetuned.png')
plt.show()

# --- Ewaluacja po fine-tuningu ---
test_loss_ft, test_acc_ft = model_inception.evaluate(test_generator_inception)
print(f'\n‚úÖ Test accuracy after fine-tuning: {test_acc_ft:.4f}')

# --- Predykcje i macierz pomy≈Çek ---
predictions_ft = model_inception.predict(test_generator_inception)
pred_classes_ft = np.argmax(predictions_ft, axis=1)
true_classes = test_generator_inception.classes

labels = list(test_generator_inception.class_indices.keys())
cm_ft = confusion_matrix(true_classes, pred_classes_ft)
errors_ft = cm_ft.sum(axis=1) - np.diag(cm_ft)

df_errors_ft = pd.DataFrame({
    'species': labels,
    'errors': errors_ft
}).sort_values(by='errors', ascending=False)

df_errors_ft.to_csv('C:/Users/pgryg/Desktop/CNN_project/errors_per_class_inception_finetuned.csv', index=False)
print("\nüìä Zapisano tabelƒô b≈Çƒôd√≥w po fine-tuningu.")

# --- Zapis b≈Çƒôdnych predykcji po fine-tuningu ---
filenames = test_generator_inception.filenames
filepaths = [os.path.join(test_path, fname) for fname in filenames]

df_all_ft = pd.DataFrame({
    'file_path': filepaths,
    'true_class': true_classes,
    'pred_class': pred_classes_ft,
    'true_label': [labels[i] for i in true_classes],
    'pred_label': [labels[i] for i in pred_classes_ft],
})

df_mistakes_ft = df_all_ft[df_all_ft['true_class'] != df_all_ft['pred_class']]
df_mistakes_ft.to_csv('C:/Users/pgryg/Desktop/CNN_project/test_errors_only_inception_finetuned.csv', index=False)
print("‚ùå B≈Çƒôdy predykcji (fine-tuning) zapisane do: test_errors_only_inception_finetuned.csv")

# --- Zapis ostatecznego modelu ---
model_inception.save('C:/Users/pgryg/Desktop/CNN_project/model_final_inception_finetuned.h5')